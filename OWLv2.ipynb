{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers \n",
    "!pip install torchvision\n",
    "!pip install torch\n",
    "!pip install pycocotools \n",
    "!pip install tqdm \n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der notwendigen Module\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell und Prozessor laden\n",
    "processor = AutoProcessor.from_pretrained(\"google/owlv2-large-patch14-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-large-patch14-ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade anpassen\n",
    "image_folder = \"   \" \n",
    "annotation_file = \"   \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Festlegen des Geräts (CPU oder GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilder sammeln\n",
    "image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith('.jpg') or img.endswith('.png')]\n",
    "\n",
    "# Laden der Ground-Truth-Annotationen\n",
    "coco_gt = COCO(annotation_file)\n",
    "\n",
    "# Erstellen des Mappings von Dateinamen zu Bild-IDs\n",
    "filename_to_image_id = {img['file_name']: img['id'] for img in coco_gt.dataset['images']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferenz vorbereiten\n",
    "results_owlv2 = []\n",
    "text_prompts = [\"animal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilder durchlaufen und verarbeiten\n",
    "for image_path in tqdm(image_paths, desc=\"Verarbeitung von Bildern\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    image_id = filename_to_image_id.get(image_filename)\n",
    "\n",
    "    if image_id is None:\n",
    "        print(f\"Bild-ID für {image_filename} nicht gefunden. Überspringe dieses Bild.\")\n",
    "        continue\n",
    "\n",
    "    # Vorverarbeitung\n",
    "    inputs = processor(images=image, text=text_prompts, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Inferenz\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-Processing\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs=outputs, threshold=0.1, target_sizes=target_sizes)[0]\n",
    "\n",
    "    # Extrahieren der Bounding Boxes, Scores und Labels\n",
    "    results_owlv2.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"boxes\": results[\"boxes\"].tolist(),\n",
    "        \"scores\": results[\"scores\"].tolist(),\n",
    "        \"labels\": results[\"labels\"].tolist(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertierung der Ergebnisse in das COCO-Format\n",
    "def convert_to_coco_format_owlv2(results, ground_truth_category_id):\n",
    "    coco_results = []\n",
    "    for result in results:\n",
    "        image_id = result[\"image_id\"]\n",
    "        for box, score in zip(result[\"boxes\"], result[\"scores\"]):\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            # Setzen der category_id auf die Ground-Truth-Kategorie-ID\n",
    "            coco_results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": ground_truth_category_id,\n",
    "                \"bbox\": [x_min, y_min, width, height],\n",
    "                \"score\": score\n",
    "            })\n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_coco_format_owlv2(results, ground_truth_category_id):\n",
    "    coco_results = []\n",
    "    for result in results:\n",
    "        image_id = result[\"image_id\"]\n",
    "        boxes = result[\"boxes\"]\n",
    "        scores = result[\"scores\"]\n",
    "        labels = result[\"labels\"] # Die Labels sind die Indizes der Text-Prompts\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            # Setzen der category_id auf die Ground-Truth-Kategorie-ID\n",
    "            category_id = ground_truth_category_id\n",
    "            coco_results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x_min, y_min, width, height],\n",
    "                \"score\": score\n",
    "            })\n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse konvertieren und speichern\n",
    "animal_category_id = 1 # Entsprechend der Ground-Truth-Annotationen\n",
    "coco_results_owlv2 = convert_to_coco_format_owlv2(results_owlv2, animal_category_id)\n",
    "\n",
    "with open(\"owlv2_results.json\", \"w\") as f:\n",
    "    json.dump(coco_results_owlv2, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation der Ergebnisse\n",
    "coco_dt_owlv2 = coco_gt.loadRes(\"owlv2_results.json\")\n",
    "coco_eval_owlv2 = COCOeval(coco_gt, coco_dt_owlv2, iouType='bbox')\n",
    "coco_eval_owlv2.evaluate()\n",
    "coco_eval_owlv2.accumulate()\n",
    "coco_eval_owlv2.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
