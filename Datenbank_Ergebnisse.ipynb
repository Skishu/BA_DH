{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "!pip install numpy\n",
    "!pip install pyarrow \n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bereinigung der Datenbank\n",
    "# Lade die Parquet-Datei von dem angegebenen Pfad\n",
    "file_path = \"   \" # Pfad zur Parquet-Datei\n",
    "\n",
    "# Lade Parquet-Datei\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Extrahiere die relevanten Spalten\n",
    "df_filtered = df[['merge_id', 'Jahr', 'Verfasser']]\n",
    "\n",
    "# Funktion zur Bereinigung der Jahr-Spalte\n",
    "def clean_year(year):\n",
    "    if pd.isna(year):\n",
    "        return np.nan\n",
    "    cleaned = re.sub(r'[\\[\\]()?;]', '', str(year)) # Entferne Sonderzeichen\n",
    "    cleaned = re.sub(r'\\b(ca\\.|um|circa|vor|nach|erschienen|nicht ermittelbar)\\b', '', cleaned, flags=re.IGNORECASE) # Entferne überflüssige Wörter\n",
    "    cleaned = re.sub(r'\\s+', '', cleaned) # Entferne überflüssige Leerzeichen\n",
    "    match = re.search(r'\\b\\d{4}\\b', cleaned)\n",
    "    return match.group(0) if match else np.nan\n",
    "\n",
    "# Bereinigen der 'Jahr'-Spalte und speichern im DataFrame\n",
    "df_filtered['Jahr'] = df_filtered['Jahr'].apply(clean_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Zeige die bereinigten eindeutigen Jahre\n",
    "unique_years_sorted = sorted(df_filtered['Jahr'].dropna().unique(), key=lambda x: int(x))\n",
    "print(\"Bereinigte eindeutige Jahre in der Datenbank:\")\n",
    "for year in unique_years_sorted:\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichere die bereinigte Datenbank an einem neuen Ort\n",
    "output_path = \"   \" # Zielpfad für die bereinigte Parquet-Datei\n",
    "df_filtered.to_parquet(output_path, index=False)\n",
    "print(f\"Bereinigte Datenbank erfolgreich gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filterung: Bücher zwischen 1800 und 1900\n",
    "filtered_df = df_filtered[(df_filtered['Jahr'].astype(str) >= '1800') & (df_filtered['Jahr'].astype(str) <= '1900')]\n",
    "\n",
    "# Extrahiere nur die Buchnummern und das Jahr\n",
    "filtered_data = filtered_df[['merge_id', 'Jahr']].to_dict(orient='records')\n",
    "\n",
    "# Speichern als JSON\n",
    "output_json_path = \"   \" # Pfad zur Ausgabe-JSON-Datei\n",
    "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "print(\"JSON-Datei wurde erfolgreich gespeichert:\", output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopiere Bücherordner\n",
    "json_path = \"   \"  # Pfad zur JSON-Datei mit Buchnummern\n",
    "source_base_path = \"   \" # Quellpfad für die Bücherordner\n",
    "destination_path = \"   \" # Zielpfad für die kopierten Ordner\n",
    "\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# Lade JSON-Datei\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "book_ids = {str(entry['merge_id']) for entry in data}\n",
    "\n",
    "# Durchlaufe alle Ordner im Dataset und kopiere relevante\n",
    "for folder_name in os.listdir(source_base_path):\n",
    "    folder_path = os.path.join(source_base_path, folder_name)\n",
    "    if folder_name in book_ids and os.path.isdir(folder_path):\n",
    "        dest_folder_path = os.path.join(destination_path, folder_name)\n",
    "        shutil.copytree(folder_path, dest_folder_path)\n",
    "        print(f\"Ordner '{folder_name}' wurde erfolgreich kopiert.\")\n",
    "\n",
    "print(\"Alle passenden Ordner wurden erfolgreich kopiert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle ein Diagramm für den Tier-Faktor\n",
    "csv_path = \"   \" # Pfad zur CSV-Datei mit Tier-Faktor-Daten\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Erscheinungsjahr'], df['Durchschnittlicher Tier-Faktor'], marker='o', linestyle='-')\n",
    "plt.xlabel('Jahr')\n",
    "plt.ylabel('Durchschnittlicher Tier-Faktor')\n",
    "plt.title('Durchschnittlicher Tier-Faktor pro Jahr (1800-1900)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Autoren von 1812\n",
    "database_path = \"   \"  # Pfad zur Datenbank-Datei\n",
    "df = pd.read_parquet(database_path)\n",
    "authors_1812 = df[df['Jahr'] == '1812']['Verfasser'].unique()\n",
    "print(\"Autoren, die 1812 veröffentlicht haben:\")\n",
    "for author in authors_1812:\n",
    "    print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Anzahl der Bücher pro Jahr\n",
    "books_per_year = df.groupby('Jahr')['merge_id'].nunique()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(books_per_year.index, books_per_year.values, marker='o', linestyle='-')\n",
    "plt.xlabel('Jahr')\n",
    "plt.ylabel('Anzahl der Bücher')\n",
    "plt.title('Anzahl der veröffentlichten Bücher pro Jahr')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereinigen von Namen in der Datenbankdatei\n",
    "def sanitize_name(name):\n",
    "    if isinstance(name, str):\n",
    "        return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "    return name\n",
    "\n",
    "cleaned_file_path = \"   \" # Pfad zur bereinigten Datei\n",
    "data = pd.read_parquet(database_path)\n",
    "\n",
    "if 'Verfasser' in data.columns:\n",
    "    data['Verfasser'] = data['Verfasser'].apply(sanitize_name)\n",
    "if 'merge_id' in data.columns:\n",
    "    data['merge_id'] = data['merge_id'].apply(sanitize_name)\n",
    "\n",
    "data.to_parquet(cleaned_file_path, index=False)\n",
    "print(f\"Die bereinigte .parquet-Datei wurde gespeichert unter: {cleaned_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
